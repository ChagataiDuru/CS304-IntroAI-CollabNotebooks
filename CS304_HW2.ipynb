{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP+ZJk7x2BK+dByzNdC8Kyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChagataiDuru/CS304-IntroAI-CollabNotebooks/blob/main/CS304_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2\n"
      ],
      "metadata": {
        "id": "AVKY9jviPFqU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrR-CcusOa4M",
        "outputId": "f0cc69fa-f135-4b14-8c0f-9ed4697a594e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Both /content/drive/MyDrive/Colab Notebooks/CS304/HW2/features_train.txt and /content/drive/MyDrive/Colab Notebooks/CS304/HW2/features_test.txt exists.\n"
          ]
        }
      ],
      "source": [
        "# Importing Necessary Libraries and File for Part 2 soft-margin SVM to handwritten digits\n",
        "from os.path import exists\n",
        "import itertools as itertools\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_data = \"/content/drive/MyDrive/Colab Notebooks/CS304/HW2/features_train.txt\"\n",
        "test_data = \"/content/drive/MyDrive/Colab Notebooks/CS304/HW2/features_test.txt\"\n",
        "\n",
        "# - Supress all warnings (Optional)\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore',\n",
        "                      category=FutureWarning)\n",
        "if exists(train_data) and exists(test_data):\n",
        "    print(f\"\\nBoth {train_data} and {test_data} exists.\")\n",
        "else:\n",
        "    print(\"Please set directory to read the files\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ],
      "metadata": {
        "id": "tovD7dUmraMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the nonlinear error surface $E(u, v)=\\left(u e^{v}-2 v e^{-u}\\right)^{2}$. We start at the point $$(u, v)=(1,1)$$ and minimize this error using gradient descent in the $u v$ space. Use $$\\eta=0.1$$ (learning rate, not step size).\n",
        "##Â Question 1 ##\n",
        "What is the partial derivative of $E(u, v)$ with respect to $u$, i.e., $\\frac{\\partial E}{\\partial u}$ ?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Error Surface:**\n",
        "\n",
        "$$E(u, v) = \\left(u e^{v}-2 v e^{-u}\\right)^{2}$$\n",
        "\n",
        "**Partial Derivative with Respect to u:**\n",
        "\n",
        "$$\\frac{\\partial E}{\\partial u} = 2\\left(e^{v}+2 v e^{-u}\\right)\\left(u e^{v}-2 v e^{-u}\\right)$$"
      ],
      "metadata": {
        "id": "DxwGV5ZoPHhD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 2##\n",
        "How many iterations (among given choices) does it take for the error $E(u, v)$ to fall below $10^{-14}$ for the first time? In your programs, make sure to use double precision to get the needed accuracy.\n",
        "\n",
        "(a) 1\n",
        "\n",
        "(b) 3\n",
        "\n",
        "(c) 5\n",
        "\n",
        "(d) 10\n",
        "\n",
        "(e) 17\n",
        "\n",
        "**Answer: d**\n",
        "\n",
        "##Question 3##\n",
        "After running enough iterations such that the error has just dropped below $10^{-14}$, what are the closest values (in Euclidean distance) among the following choices to the final $(u, v)$ you got in problem 2 ?\n",
        "\n",
        "(a) $(1.000,1.000)$\n",
        "\n",
        "(b) $(0.713,0.045)$\n",
        "\n",
        "(c) $(0.016,0.112)$\n",
        "\n",
        "(d) $(-0.083,0.029)$\n",
        "\n",
        "(e) $(0.045,0.024)$\n",
        "\n",
        "\n",
        "**Answer: e**\n",
        "\n",
        "### Below the code ###"
      ],
      "metadata": {
        "id": "NyEHhvaCbsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_surface(u, v):\n",
        "    return (u * np.exp(v) - 2 * v * np.exp(-u))**2\n",
        "\n",
        "def dE_du(u, v):\n",
        "    return 2 * (np.exp(v) + 2 * v * np.exp(-u)) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
        "\n",
        "def dE_dv(u, v):\n",
        "    return 2 * u * np.exp(v) * (u * np.exp(v) - 2 * v * np.exp(-u))\n",
        "\n",
        "def gradient_descent(E, dE_du, dE_dv, u_0, v_0, learning_rate = 0.1,\n",
        "                     stopping_threshold = 1e-14):\n",
        "    u = u_0\n",
        "    v = v_0\n",
        "    iterations = 0\n",
        "\n",
        "    while E(u, v) >= stopping_threshold:\n",
        "        u_new = u - learning_rate * dE_du(u, v)\n",
        "        v_new = v - learning_rate * dE_dv(u, v)\n",
        "        u = u_new\n",
        "        v = v_new\n",
        "        iterations += 1\n",
        "\n",
        "    return u, v, iterations\n",
        "\n",
        "u_0 = 1\n",
        "v_0 = 1\n",
        "eta = 0.1\n",
        "threshold = 10**(-14)\n",
        "\n",
        "final_u, final_v, iterations = gradient_descent(error_surface, dE_du, dE_dv, u_0, v_0, eta, threshold)\n",
        "\n",
        "print(\"Final values (u, v): \", final_u, final_v)\n",
        "print(\"Iterations to converge: \", iterations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk9h3GXVdG1u",
        "outputId": "b81f6ac7-3dbc-45ac-a85f-d046ef63e0e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final values (u, v):  -0.3153488929785157 -0.10369822395113008\n",
            "Iterations to converge:  49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Question 4##\n",
        "Now, we will compare the performance of \"coordinate descent.\" In each iteration, we have two steps along the 2 coordinates. Step 1 is to move only along the $u$ coordinate to reduce the error (assume first-order approximation holds like in gradient descent), and step 2 is to reevaluate and move only along the $v$ coordinate to reduce the error (again, assume first-order approximation holds). Use the same learning rate of $\\eta=0.1$ as we did in gradient descent. What will the error $E(u, v)$ be closest to after 15 full iterations (30 steps)?\n",
        "\n",
        "(a) $10^{-1}$\n",
        "\n",
        "(b) $10^{-7}$\n",
        "\n",
        "(c) $10^{-14}$\n",
        "\n",
        "(d) $10^{-17}$\n",
        "\n",
        "(e) $10^{-20}$\n",
        "\n",
        "**Answer: a**\n"
      ],
      "metadata": {
        "id": "Gugy7KzupExm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Support Vector Machines"
      ],
      "metadata": {
        "id": "G2z6Juy1rZMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1.a Load the two datasets##"
      ],
      "metadata": {
        "id": "l9wuwCC2qQjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "X_train = np.loadtxt(train_data, usecols=(1, 2))  # Features\n",
        "y_train = np.loadtxt(train_data, usecols=(0), dtype=int)  # Labels\n",
        "\n",
        "# Load test data\n",
        "X_test = np.loadtxt(test_data, usecols=(1, 2))\n",
        "y_test = np.loadtxt(test_data, usecols=(0), dtype=int)"
      ],
      "metadata": {
        "id": "Okxh5-vqe2Tp",
        "outputId": "65f935f1-a765-460d-b85d-79ab3d09d50b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6a1d08ab9186>:3: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
            "    * make sure the original data is stored as integers.\n",
            "    * use the `converters=` keyword argument.  If you only use\n",
            "      NumPy 1.23 or later, `converters=float` will normally work.\n",
            "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
            "      floating point and then convert it.  (On all NumPy versions.)\n",
            "  (Deprecated NumPy 1.23)\n",
            "  y_train = np.loadtxt(train_data, usecols=(0), dtype=int)  # Labels\n",
            "<ipython-input-3-6a1d08ab9186>:7: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
            "    * make sure the original data is stored as integers.\n",
            "    * use the `converters=` keyword argument.  If you only use\n",
            "      NumPy 1.23 or later, `converters=float` will normally work.\n",
            "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
            "      floating point and then convert it.  (On all NumPy versions.)\n",
            "  (Deprecated NumPy 1.23)\n",
            "  y_test = np.loadtxt(test_data, usecols=(0), dtype=int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1.b Display the shape of both training and test data."
      ],
      "metadata": {
        "id": "0PwOigEusBtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of datasets\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n",
        "\n",
        "print(\"Training set shape:\", y_train.shape)\n",
        "print(\"Test set shape:\", y_test.shape)\n",
        "#  ---------------------"
      ],
      "metadata": {
        "id": "InQ5MAudsXmO",
        "outputId": "a94ead4a-2b0e-4002-9882-d3b4cf3e0e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (7291, 2)\n",
            "Test set shape: (2007, 2)\n",
            "Training set shape: (7291,)\n",
            "Test set shape: (2007,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2 train and evaluate a binary classifier"
      ],
      "metadata": {
        "id": "r4aOezF4tRTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_binary_classifier(X, y, X_test, y_test, digit1, digit2, kernel='linear', C=1.0, degree=3):\n",
        "    # Extract binary labels\n",
        "    binary_mask = np.logical_or(y == digit1, y == digit2)\n",
        "    y_binary = np.where(y[binary_mask] == digit1, 1, -1)\n",
        "    X_binary = X[binary_mask]\n",
        "\n",
        "    # One-vs-one\n",
        "    svm_clf = SVC(kernel=kernel, C=C, degree=degree, gamma='auto')\n",
        "    svm_clf.fit(X_binary, y_binary)\n",
        "\n",
        "    # Evaluate on training data\n",
        "    y_pred_train = svm_clf.predict(X_binary)\n",
        "    train_error = 1 - accuracy_score(y_binary, y_pred_train)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_test = svm_clf.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(np.where(y_test == digit1, 1, np.where(y_test == digit2, -1, 0)), y_pred_test)\n",
        "\n",
        "    return svm_clf, train_error, test_error"
      ],
      "metadata": {
        "id": "gkn07rBrtYmg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Problem 1 - With C=0.01 and Q=2, which classifier has the highest E_train?\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "osXeeE5HuRdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest_train_error = 0\n",
        "highest_train_error_digit = None\n",
        "for digit1, digit2 in itertools.combinations(range(10), 2):\n",
        "    svm_clf, train_error, _ = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                                      digit1, digit2, kernel='poly', C=0.01, degree=2)\n",
        "    if train_error > highest_train_error:\n",
        "        highest_train_error = train_error\n",
        "        highest_train_error_digit = (digit1, digit2)\n",
        "\n",
        "print(f\"Classifier with highest E_train: {highest_train_error_digit}\")"
      ],
      "metadata": {
        "id": "eOCZVi13t1vS",
        "outputId": "85fe492f-6265-41e5-f310-6d119cb1923e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier with highest E_train: (4, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mjb4sbNNuR0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2: With C=0.01 and Q=2, which classifier has the lowest E_train?\n",
        "lowest_train_error = 1\n",
        "lowest_train_error_digit = None\n",
        "for digit1, digit2 in itertools.combinations(range(10), 2):\n",
        "    svm_clf, train_error, _ = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                                      digit1, digit2, kernel='poly', C=0.01, degree=2)\n",
        "    if train_error < lowest_train_error:\n",
        "        lowest_train_error = train_error\n",
        "        lowest_train_error_digit = (digit1, digit2)\n",
        "\n",
        "print(f\"Classifier with lowest E_train: {lowest_train_error_digit}\")\n",
        "\n",
        "# Problem 3: Comparing the two selected classifiers from Problems 1 and 2, which of the following values is the closest to the difference between the number of support vectors of these two classifiers?\n",
        "clf1, _, _ = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                     highest_train_error_digit[0], highest_train_error_digit[1],\n",
        "                                     kernel='poly', C=0.01, degree=2)\n",
        "clf2, _, _ = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                     lowest_train_error_digit[0], lowest_train_error_digit[1],\n",
        "                                     kernel='poly', C=0.01, degree=2)\n",
        "\n",
        "support_vector_diff = abs(len(clf1.support_vectors_) - len(clf2.support_vectors_))\n",
        "closest_choice = min([(600, abs(600 - support_vector_diff)), (1200, abs(1200 - support_vector_diff)),\n",
        "                      (1800, abs(1800 - support_vector_diff)), (2400, abs(2400 - support_vector_diff)),\n",
        "                      (3000, abs(3000 - support_vector_diff))], key=lambda x: x[1])[0]\n",
        "\n",
        "print(f\"Closest value to the difference between support vectors: {closest_choice}\")\n",
        "\n",
        "# Problem 4: Consider the 1 versus 5 classifier with Q=2 and C â {0.001, 0.01, 0.1, 1}. Which of the following statements is correct?\n",
        "C_values = [0.001, 0.01, 0.1, 1]\n",
        "for C in C_values:\n",
        "    svm_clf, train_error, test_error = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                                               1, 5, kernel='poly', C=C, degree=2)\n",
        "    print(f\"C = {C}, Number of support vectors: {len(svm_clf.support_vectors_)}, E_train: {train_error}, E_test: {test_error}\")\n",
        "\n",
        "# Problem 5: In the 1 versus 5 classifier, comparing Q=2 with Q=5, which of the following statements is correct?\n",
        "svm_clf_Q2, train_error_Q2, test_error_Q2 = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                                                    1, 5, kernel='poly', C=0.0001, degree=2)\n",
        "svm_clf_Q5, train_error_Q5, test_error_Q5 = train_binary_classifier(X_train, y_train, X_test, y_test,\n",
        "                                                                    1, 5, kernel='poly', C=0.0001, degree=5)\n",
        "\n",
        "print(f\"C=0.0001, Q=2: E_train={train_error_Q2}, Q=5: E_train={train_error_Q5}\")\n",
        "print(f\"C=0.0001, Q=2: Number of support vectors={len(svm_clf_Q2.support_vectors_)}, Q=5: Number of support vectors={len(svm_clf_Q5.support_vectors_)}\")\n",
        "\n",
        "# Repeat for other C values (0.001, 0.01, 1)"
      ],
      "metadata": {
        "id": "uJiRz28Pue7C",
        "outputId": "5376691b-51ed-4508-de5c-2450b270b051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier with lowest E_train: (1, 5)\n",
            "Closest value to the difference between support vectors: 1200\n",
            "C = 0.001, Number of support vectors: 152, E_train: 0.007046764894298563, E_test: 0.7927254608868959\n",
            "C = 0.01, Number of support vectors: 54, E_train: 0.004484304932735439, E_test: 0.7927254608868959\n",
            "C = 0.1, Number of support vectors: 28, E_train: 0.004484304932735439, E_test: 0.7927254608868959\n",
            "C = 1, Number of support vectors: 25, E_train: 0.004484304932735439, E_test: 0.7927254608868959\n",
            "C=0.0001, Q=2: E_train=0.022421524663677084, Q=5: E_train=0.006406149903907754\n",
            "C=0.0001, Q=2: Number of support vectors=510, Q=5: Number of support vectors=42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation\n",
        "In the next two problems, we will experiment with 10 -fold cross validation for the polynomial kernel. Because $E_{c v}$ is a random variable that depends on the random partition of the data, we will try 100 runs with different partitions and base our answer on how many runs lead to a particular choice."
      ],
      "metadata": {
        "id": "N9RKiTMbvjfa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvSpRa2Lv5rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RBF Kernel\n",
        "Consider the radial basis function (RBF) kernel $K\\left(\\boldsymbol{x}_{n}, \\boldsymbol{x}_{m}\\right)=\\exp \\left(-\\left\\|\\boldsymbol{x}_{n}-\\boldsymbol{x}_{m}\\right\\|^{2}\\right)$ in the soft-margin SVM approach. Focus on the 1 versus 5 classifier."
      ],
      "metadata": {
        "id": "u5T6o0uovUkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_binary_classifier_rbf(X, y, X_test, y_test, digit1, digit2, C):\n",
        "    # Extract binary labels\n",
        "    binary_mask = np.logical_or(y == digit1, y == digit2)\n",
        "    y_binary = np.where(y[binary_mask] == digit1, 1, -1)\n",
        "    X_binary = X[binary_mask]\n",
        "\n",
        "    # Train SVM with RBF kernel\n",
        "    svm_clf = SVC(kernel='rbf', C=C, gamma='auto')\n",
        "    svm_clf.fit(X_binary, y_binary)\n",
        "\n",
        "    # Evaluate on training data\n",
        "    y_pred_train = svm_clf.predict(X_binary)\n",
        "    train_error = 1 - accuracy_score(y_binary, y_pred_train)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_test = svm_clf.predict(X_test)\n",
        "    test_error = 1 - accuracy_score(np.where(y_test == digit1, 1, np.where(y_test == digit2, -1, 0)), y_pred_test)\n",
        "\n",
        "    return svm_clf, train_error, test_error"
      ],
      "metadata": {
        "id": "BEX1Rcr4vgLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 7: Which of the following values of C results in the lowest E_train?\n",
        "C_values = [0.01, 1, 100, 10000, 1000000]\n",
        "lowest_train_error = 1\n",
        "best_C_train = None\n",
        "for C in C_values:\n",
        "    svm_clf, train_error, _ = train_binary_classifier_rbf(X_train, y_train, X_test, y_test, 1, 5, C)\n",
        "    if train_error < lowest_train_error:\n",
        "        lowest_train_error = train_error\n",
        "        best_C_train = C\n",
        "\n",
        "print(f\"The value of C that results in the lowest E_train is: {best_C_train}\")\n",
        "\n",
        "# Problem 8: Which of the following values of C results in the lowest E_test?\n",
        "lowest_test_error = 1\n",
        "best_C_test = None\n",
        "for C in C_values:\n",
        "    svm_clf, _, test_error = train_binary_classifier_rbf(X_train, y_train, X_test, y_test, 1, 5, C)\n",
        "    if test_error < lowest_test_error:\n",
        "        lowest_test_error = test_error\n",
        "        best_C_test = C\n",
        "\n",
        "print(f\"The value of C that results in the lowest E_test is: {best_C_test}\")"
      ],
      "metadata": {
        "id": "QPx_0Hnrv987"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}